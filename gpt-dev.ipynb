{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import List, Tuple, Optional, Dict, Type\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexandro Barajas: Image_Placeholder\n",
      "Gustavo Fuentes: changos, no pos en definitiva si necesitas una de esas\n",
      "Alexandro Barajas: Image_Placeholder\n",
      "Gustavo Fuentes: ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£\n",
      "Omar Valerdi: Image_Placeholder\n",
      "Alexandro Barajas: Image_Placeholder\n",
      "Gustavo Fuentes: Image_Placeholder\n",
      "Gustavo Fuentes: https://youtu.be/SFj9u42et7o?t=189\n",
      "Gustavo Fuentes: @818052144488 por quÃ© les ponen \"chips\" al arroz?\n",
      "Omar Bazavilvazo: 1) nadie en la vida le pone chips. Al arroz . Hay algo llamado furikake para los niÃ±os\n",
      "Omar Bazavilvazo: 2) jamÃ³n? Letra en romanji? Wow nunca conocÃ­ un japonÃ©s que me invitara algo con jamÃ³n , que paciencia de hacer eso â€¦ pero sabe Manuel tiene mÃ¡s tiempo allÃ¡ , capaz que ya cambiaron las cosas\n",
      "Ignacio PÃ©rez BermÃºdez: Image_Placeholder\n",
      "Gustavo Fuentes: No debe ser tan incomun ya que hay un utensilio para triturar chips\n",
      "Gustavo Fuentes: Las letras me imagino que son novelty\n",
      "Omar Valerdi: Image_Placeholder\n",
      "Ignacio PÃ©rez BermÃºdez: Image_Placeholder\n",
      "Alexandro Barajas: Que esperant\n"
     ]
    }
   ],
   "source": [
    "with open('ApoteosisWhatsAppChat.txt', 'r') as f:\n",
    "    input_text = f.read()\n",
    "\n",
    "print(input_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t\\n !\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz|}~\\xa0Â¡ÂªÂ«Â°Â´Â»Â¿ÃÃ‰ÃÃ‘Ã“ÃšÃ Ã¡Ã¤Ã§Ã©Ã­Ã®Ã±Ã³Ã¶Ã·ÃºÃ¼ÄºÅ„Å•Å›\\u200b\\u200d\\u200eâ€”â€˜â€™â€œâ€â€¢â€¦\\u202fâ€¼â´â‚‰â†‘âˆšâˆ§â–¶â—‡â˜â˜ â˜¹â˜ºâ™‚â™¥â™¬âš¡âœ…âœ‹âœŒâœ”âŒâ¤ã€ã€‚ã‚ã„ã†ãŒã“ã•ã—ã™ãŸã ã£ã¤ã§ã¨ã©ãªã«ã®ã¯ã»ã¾ã¿ã‚‡ã‚Šã‚‹ã‚Œã‚’ã‚“ã‚¡ã‚¢ã‚¤ã‚¨ã‚ªã‚­ã‚°ã‚±ã‚³ã‚´ã‚·ã‚¸ã‚¹ã‚»ãƒƒãƒ†ãƒ‡ãƒˆãƒãƒ‘ãƒ’ãƒ“ãƒ•ãƒ—ãƒãƒ¡ãƒ©ãƒ«ãƒ³ãƒ¼ä¸€ä¸ƒä¸‰ä¸Šä¸­äº†äº•äº¤äººä»€ä»˜ä»¶ä½ä½å€‹å„„å‰ŠåŒ–å‘å’Œå“¥åœ°å ±å¢¨å¤‰å­å­—å­¦å°‘å±‹å´å¸¸å¹³å¹´å½“æ‚Ÿæ‚ªæƒ…æ„›æ‡¸æŠ•æŠ¹æŒ‡æ–‡æ—¥æ›´æœ€æœ¨æœ¬æœ±æ‰æ ¹æ²¢æ³Šæ³¢æ´»æµæ·³æ·»æ¼æ¼¢ç„¶ç”Ÿç”¨ç—›ç™ºç›®çŸ³ç¥¨ç©ºç±³ç´ ç·¨ç½®è‚‰èŠ¸è‹±èŒ¶è“è¥¿è¦‹èªèª­è³è¼ªé€ é“é¸é…¸éˆ´é™¢é™¤é›†é›»éœ²éŸ³é¢¨ï¸ï¼ï¼ˆï¼‰ï¼Ÿğ‘ªğ’‚ğ’„ğ’‰ğ’Šğ’ğ’”ğ’•ğ˜¼ğ™’ğ™–ğ™šğ™ğ™¡ğ™©ğŸ‡§ğŸ‡ªğŸ‡²ğŸ‡·ğŸ‡¸ğŸ‡½ğŸŒ²ğŸŒ¸ğŸŒ¹ğŸğŸ”ğŸ•ğŸ°ğŸ·ğŸºğŸ»ğŸ¾ğŸ¿ğŸğŸ‚ğŸ„ğŸ…ğŸˆğŸ‰ğŸŠğŸ»ğŸ¼ğŸ½ğŸ¾ğŸ¢ğŸ§ğŸ¯ğŸ¶ğŸ·ğŸ»ğŸ‘€ğŸ‘ğŸ‘†ğŸ‘‰ğŸ‘ŒğŸ‘ğŸ‘ğŸ‘ğŸ‘¨ğŸ‘´ğŸ‘»ğŸ‘½ğŸ’ŠğŸ’œğŸ’£ğŸ’©ğŸ’«ğŸ’¯ğŸ’°ğŸ“ğŸ“¸ğŸ”¥ğŸ–ğŸ˜€ğŸ˜ğŸ˜‚ğŸ˜ƒğŸ˜„ğŸ˜…ğŸ˜†ğŸ˜ˆğŸ˜‰ğŸ˜ŠğŸ˜‹ğŸ˜ŒğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜‘ğŸ˜’ğŸ˜“ğŸ˜”ğŸ˜•ğŸ˜–ğŸ˜›ğŸ˜œğŸ˜ğŸ˜ğŸ˜ ğŸ˜¡ğŸ˜¢ğŸ˜£ğŸ˜¤ğŸ˜¦ğŸ˜§ğŸ˜¨ğŸ˜«ğŸ˜¬ğŸ˜­ğŸ˜®ğŸ˜°ğŸ˜±ğŸ˜²ğŸ˜³ğŸ˜´ğŸ˜µğŸ˜¶ğŸ™‚ğŸ™ƒğŸ™„ğŸ™‡ğŸ™ˆğŸ™‰ğŸ™ŠğŸ™‹ğŸ™ğŸš€ğŸš§ğŸš¨ğŸš¬ğŸ›‘ğŸ¤ğŸ¤“ğŸ¤”ğŸ¤—ğŸ¤˜ğŸ¤ŸğŸ¤¡ğŸ¤¢ğŸ¤£ğŸ¤¤ğŸ¤¦ğŸ¤¨ğŸ¤©ğŸ¤ªğŸ¤¬ğŸ¤­ğŸ¤®ğŸ¤¯ğŸ¤·ğŸ¥‚ğŸ¥ƒğŸ¥šğŸ¥°ğŸ¥±ğŸ¥²ğŸ¥³ğŸ¥µğŸ¥¶ğŸ¥¹ğŸ¦ğŸ§ğŸ§ğŸª„ğŸª…ğŸ«‚ğŸ«¢ğŸ«¤ğŸ«¥'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Total unique characters: 492'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the unique characters in the text\n",
    "chars_in_text = sorted(list(set(input_text)))\n",
    "vocab_size = len(chars_in_text)\n",
    "display(''.join(chars_in_text))\n",
    "display(f'Total unique characters: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map from chars to int, and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_int = {}\n",
    "int_to_char = {}\n",
    "\n",
    "for i, c in enumerate(chars_in_text):\n",
    "    char_to_int[c] = i\n",
    "    int_to_char[i] = c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to encode and decode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 75, 2, 79, 67, 79, 67, 2, 79, 71, 2, 79, 75, 79, 67]\n",
      "Mi mama me mima\n"
     ]
    }
   ],
   "source": [
    "def encode(text: str) -> List[int]:\n",
    "    return [char_to_int[c] for c in text]\n",
    "\n",
    "def decode(encoded_text: List[int]) -> str:\n",
    "    return ''.join([int_to_char[i] for i in encoded_text])\n",
    "\n",
    "# Test them\n",
    "encoded_text = encode('Mi mama me mima')\n",
    "print(encoded_text)\n",
    "\n",
    "print(decode(encoded_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the entire text and put it in a tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2461096])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([35, 78, 71, 90, 67, 80, 70, 84, 81,  2, 36, 67, 84, 67, 76, 67, 85, 28,\n",
       "         2, 43, 79, 67, 73, 71, 65, 50, 78, 67, 69, 71, 74, 81, 78, 70, 71, 84,\n",
       "         1, 41, 87, 85, 86, 67, 88, 81,  2, 40, 87, 71, 80, 86, 71, 85, 28,  2,\n",
       "        69, 74, 67, 80, 73, 81, 85, 14,  2, 80, 81,  2, 82, 81, 85,  2, 71, 80,\n",
       "         2, 70, 71, 72, 75, 80, 75, 86, 75, 88, 67,  2, 85, 75,  2, 80, 71, 69,\n",
       "        71, 85, 75, 86, 67, 85,  2, 87, 80, 67])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_tensor = torch.LongTensor(encode(input_text))\n",
    "display(text_tensor.shape)\n",
    "display(text_tensor[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Using 90% of the text as training, the rest as validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35, 78, 71, 90, 67, 80, 70, 84, 81,  2, 36, 67, 84, 67, 76, 67, 85, 28,\n",
       "         2, 43, 79, 67, 73, 71, 65, 50, 78, 67, 69, 71, 74, 81, 78, 70, 71, 84,\n",
       "         1, 41, 87, 85, 86, 67, 88, 81,  2, 40, 87, 71, 80, 86, 71, 85, 28,  2,\n",
       "        69, 74, 67, 80, 73, 81, 85, 14,  2, 80, 81,  2, 82, 81, 85,  2, 71, 80,\n",
       "         2, 70, 71, 72, 75, 80, 75, 86, 75, 88, 67,  2, 85, 75,  2, 80, 71, 69,\n",
       "        71, 85, 75, 86, 67, 85,  2, 87, 80, 67])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 71,  76,  67,  80,  70,  84,  81,   2,  10,  45,  75,  79,  11,   2,\n",
       "         50, 114,  84,  71,  92,  28,   2,  54,  81,  70,  81,   2,  71,  85,\n",
       "         86,  67,   2,  74,  71,  69,  74,  81,   2,  69,  81,  80,   2,  78,\n",
       "         67,  85,   2,  82,  67,  86,  67,  85,  14,   2,  83,  87, 114,   2,\n",
       "         74,  67,  91,   2,  69,  81,  79,  82,  78,  71,  86,  81,   2,  81,\n",
       "          2,  83,  87,  71,   2,  72,  87,  80,  69,  75,  81,  80,  71,   2,\n",
       "         68,  75,  71,  80,  33,   1,  41,  87,  85,  86,  67,  88,  81,   2,\n",
       "         40,  87])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split_point = int(0.9 * len(text_tensor))\n",
    "train_data = text_tensor[:split_point]\n",
    "validation_data = text_tensor[split_point:]\n",
    "\n",
    "display(train_data[:100])\n",
    "display(validation_data[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "# Make results reproduceable\n",
    "torch.manual_seed(8888)\n",
    "batch_size = 256\n",
    "# Maximum context length for predictions\n",
    "context_size = 256\n",
    "learning_rate = 3e-4\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class SplitType(Enum):\n",
    "    train = 'TRAIN'\n",
    "    validation = 'VALIDATION'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split_type: SplitType) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    working_data = train_data if split_type == SplitType.train else validation_data\n",
    "    indices = torch.randint(len(working_data) - context_size, (batch_size, ))\n",
    "    inputs = torch.stack([working_data[i : i + context_size] for i in indices])\n",
    "    targets = torch.stack([working_data[i + 1 : i + context_size + 1] for i in indices])\n",
    "    \n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model: Type[nn.Module], eval_iterations: int) -> Dict[float, float]:\n",
    "    estimated_loss = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split_type in SplitType:\n",
    "        split_loss = torch.zeros(eval_iterations)\n",
    "        for k in range(eval_iterations):\n",
    "            inputs, targets = get_batch(split_type)\n",
    "            _, loss = model(inputs, targets)\n",
    "            split_loss[k] = loss.item()\n",
    "        \n",
    "        estimated_loss[split_type] = split_loss.mean()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return estimated_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First try with Bigrams\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        # Embedding table\n",
    "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            inputs: torch.Tensor,\n",
    "            targets: Optional[torch.Tensor] = None\n",
    "            ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "\n",
    "        logits = self.embedding_table(inputs)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if targets is not None:\n",
    "            # Reshape as Pytorch expects\n",
    "            b, t, c = logits.shape\n",
    "            logits = logits.view(b*t, c)\n",
    "            targets = targets.view(b*t)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # The return value is (batch_size, context_size, vocab_size)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generates up to max_new_tokens of text by predicting what comes after the inputs\n",
    "        \"\"\"\n",
    "        prediction_appended = inputs\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(prediction_appended)\n",
    "            # We are interested only in the last time step\n",
    "            logits = logits[:, -1, :]\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            # Sample\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            # Append to the current input\n",
    "            prediction_appended = torch.cat((prediction_appended, next_token), dim=1)\n",
    "\n",
    "        return prediction_appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65536, 492])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(6.6914, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Expected loss: 6.198478716492308'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigram_language_model = BigramLanguageModel(vocab_size).to(device)\n",
    "inputs, targets = get_batch(SplitType.train)\n",
    "output, loss = bigram_language_model(inputs, targets)\n",
    "display(output.shape, loss)\n",
    "display(f'Expected loss: {-np.log(1/vocab_size)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model: Type[nn.Module], max_new_tokens: int = 100) -> str:\n",
    "    inputs = torch.ones((1, 1), dtype=torch.long).to(device)\n",
    "    encoded_generated_text = model.generate(inputs, max_new_tokens=max_new_tokens)\n",
    "    return decode(encoded_generated_text[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\tğŸ»â™‚5ğŸ¾(ãƒ—Ã©+hğŸ‘Vã„gâ€æ ¹å¸¸,ğŸ¯&ã‚­ğ’Š!âœ‹Ã¡ÃšğŸ«¢i(ãƒ—ğŸ¶k_å±‹ğŸ‰ğŸ»ğŸ™ŠğŸ˜ˆã¤Aé¢¨;æµäº•ã‚»âœ”ä»˜ğŸ·èª3ğŸ˜„ãƒ—Mã‚¢ğŸ‘½Å›çŸ³ğŸ¤¡ğ’„fğŸ˜¨ï¼Ÿdä¸ƒâ˜ è‹±\\xa0WğŸš§ğŸ¥µğŸ‘å¹³ğŸ™Šæ›´âœ”ğŸ˜ğŸ˜•ç™ºğŸ˜†\\tğŸ˜•}ã‚¹aÃº?ğŸ‚ğŸ˜ŠpğŸ˜¨ã„ğ™ğŸ˜æ³¢ãŸdå½“XYÃ‘$'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate!\n",
    "generate(bigram_language_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033bc45042b343fba79099bae3f13ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 0: Train loss: 5.624845504760742, Validation loss: 5.623149871826172'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 1000: Train loss: 4.147069454193115, Validation loss: 4.134329795837402'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 2000: Train loss: 3.2229230403900146, Validation loss: 3.199406147003174'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 3000: Train loss: 2.760833978652954, Validation loss: 2.7316036224365234'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 4000: Train loss: 2.5586998462677, Validation loss: 2.527670383453369'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 5000: Train loss: 2.4757778644561768, Validation loss: 2.443186044692993'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 6000: Train loss: 2.438324451446533, Validation loss: 2.406372547149658'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 7000: Train loss: 2.4204578399658203, Validation loss: 2.38958740234375'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 8000: Train loss: 2.4117178916931152, Validation loss: 2.3797426223754883'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 9000: Train loss: 2.4053385257720947, Validation loss: 2.3742458820343018'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "optimizer = torch.optim.AdamW(bigram_language_model.parameters(), lr=1e-3)\n",
    "\n",
    "max_iterations = 10000\n",
    "eval_iterations = 1000\n",
    "\n",
    "for iteration in tqdm_notebook(range(max_iterations)):\n",
    "\n",
    "    if iteration % eval_iterations == 0:\n",
    "        evaluated_loss = estimate_loss(bigram_language_model, eval_iterations)\n",
    "        display(f'Step {iteration}: Train loss: {evaluated_loss[SplitType.train]}, Validation loss: {evaluated_loss[SplitType.validation]}')\n",
    "\n",
    "    inputs, targets = get_batch(SplitType.train)\n",
    "\n",
    "    logits, loss = bigram_language_model(inputs, targets)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t1?\\nMardrstavantğŸ¤¨â€¼ğ’ãƒ¼ã‚¸ã¯ç·¨é›†ã•ã•ã‚Œã¾ã—ãŸ\\nT&s: ajant\\nAlldehe ndr Ales eho ğŸ˜ğŸ˜­ğŸ”¥ãªÅ•ğ‘ªğŸ˜¤ğŸ’«ã€‚Jabese a ma.\\nMFuevoue gez Po '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate(bigram_language_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty much the limit of the model without tinkering anything else.\n",
    "\n",
    "Now, let's plug attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTENTION_HEAD_DIMENSIONS = 384\n",
    "dropout_rate = 0.2\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embedding_dimensions: int, head_dimensions: int):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embedding_dimensions, head_dimensions)\n",
    "        self.query = nn.Linear(n_embedding_dimensions, head_dimensions)\n",
    "        self.value = nn.Linear(n_embedding_dimensions, head_dimensions)\n",
    "\n",
    "        # Since this is not a parameter, it's registered as a buffer\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, time_pos, dimensions = inputs.shape\n",
    "        key = self.key(inputs)\n",
    "        query = self.query(inputs)\n",
    "\n",
    "        # Calculating affinities\n",
    "        affinities = query @ key.transpose(-2, -1) * dimensions ** -0.5\n",
    "        affinities = affinities.masked_fill(self.tril[:time_pos, :time_pos] == 0, float('-inf'))\n",
    "        affinities = F.softmax(affinities, dim=-1)\n",
    "        affinities = self.dropout(affinities)\n",
    "\n",
    "        value = self.value(inputs)\n",
    "        outputs = affinities @ value\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads: int, n_embedding_dimensions: int, attention_head_dimensions: int) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(n_embedding_dimensions, attention_head_dimensions) for _ in range(n_heads)] \n",
    "        )\n",
    "        self.projection = nn.Linear(attention_head_dimensions * n_heads, attention_head_dimensions * n_heads)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, inputs) -> torch.Tensor:\n",
    "        outputs = torch.cat([head(inputs) for head in self.heads], dim=-1)\n",
    "        outputs = self.projection(outputs)\n",
    "        outputs = self.dropout(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, attention_head_dimensions: int) -> None:\n",
    "        super().__init__()\n",
    "        self.forward_net = nn.Sequential(\n",
    "            nn.Linear(attention_head_dimensions, 4 * attention_head_dimensions),\n",
    "            nn.ReLU(),\n",
    "            # Projection layer\n",
    "            nn.Linear(4 * attention_head_dimensions, attention_head_dimensions),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward_net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A transformer block for communication -> computation\"\"\"\n",
    "\n",
    "    def __init__(self, attention_head_dimensions: int, n_attention_heads: int, n_embedding_dimensions: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = attention_head_dimensions // n_attention_heads\n",
    "        self.self_attention_heads = MultiHeadAttention(\n",
    "            n_attention_heads, n_embedding_dimensions, head_size\n",
    "        )\n",
    "        self.forward_net = FeedForward(attention_head_dimensions)\n",
    "        self.layer_norm1 = nn.LayerNorm(attention_head_dimensions)\n",
    "        self.layer_norm2 = nn.LayerNorm(attention_head_dimensions)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        # The addition is for residual connections\n",
    "        outputs = outputs + self.self_attention_heads(self.layer_norm1(inputs))\n",
    "        outputs = outputs + self.forward_net(self.layer_norm2(outputs))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelWithAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size: int, \n",
    "            context_size: int,\n",
    "            n_embedding_dimensions: int = 384,\n",
    "            n_attention_heads: int = 6,\n",
    "            n_transformer_blocks: int = 6\n",
    "        ):\n",
    "        super().__init__()\n",
    "        # Embedding table\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embedding_dimensions)\n",
    "        # Position as embedding. Transformers don't have a means to know which element of a sequence\n",
    "        # they are working with. This is learned in this table\n",
    "        self.position_embedding_table = nn.Embedding(context_size, n_embedding_dimensions)\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(ATTENTION_HEAD_DIMENSIONS, n_attention_heads, n_embedding_dimensions)\n",
    "                for _ in range(n_transformer_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(ATTENTION_HEAD_DIMENSIONS)\n",
    " \n",
    "        self.self_attention_heads = MultiHeadAttention(\n",
    "            n_attention_heads, n_embedding_dimensions, ATTENTION_HEAD_DIMENSIONS // n_attention_heads\n",
    "        )\n",
    "        self.forward_net = FeedForward(ATTENTION_HEAD_DIMENSIONS)\n",
    "        # Linear layer to go from embeddings to logits\n",
    "        self.lm_head = nn.Linear(ATTENTION_HEAD_DIMENSIONS, vocab_size)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            inputs: torch.Tensor,\n",
    "            targets: Optional[torch.Tensor] = None\n",
    "        ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \n",
    "        batch_size, time_pos = inputs.shape\n",
    "        token_embeddings = self.embedding_table(inputs)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(time_pos, device=device))\n",
    "        head_inputs = token_embeddings + position_embeddings\n",
    "        head_inputs = self.self_attention_heads(head_inputs)\n",
    "        head_inputs = self.forward_net(head_inputs)\n",
    "        logits = self.lm_head(head_inputs)  # (batch size, time, embedding_dimensions)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if targets is not None:\n",
    "            # Reshape as Pytorch expects\n",
    "            b, t, c = logits.shape\n",
    "            logits = logits.view(b*t, c)\n",
    "            targets = targets.view(b*t)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # The return value is (batch_size, context_size, vocab_size)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generates up to max_new_tokens of text by predicting what comes after the inputs\n",
    "        \"\"\"\n",
    "        prediction_appended = inputs\n",
    "        for _ in range(max_new_tokens):\n",
    "            inputs_cropped = prediction_appended[:, -context_size:]\n",
    "            logits, _ = self(inputs_cropped)\n",
    "            # We are interested only in the last time step\n",
    "            logits = logits[:, -1, :]\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            # Sample\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            # Append to the current input\n",
    "            prediction_appended = torch.cat((prediction_appended, next_token), dim=1)\n",
    "\n",
    "        return prediction_appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f03827669b4862998e7e75c4100025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 0: Train loss: 6.191287040710449, Validation loss: 6.191257476806641'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 1000: Train loss: 1.4562420845031738, Validation loss: 1.4608640670776367'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 2000: Train loss: 1.2663781642913818, Validation loss: 1.2786730527877808'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 3000: Train loss: 1.1884504556655884, Validation loss: 1.2107690572738647'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 4000: Train loss: 1.1465976238250732, Validation loss: 1.1751075983047485'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 5000: Train loss: 1.1217700242996216, Validation loss: 1.1556854248046875'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 6000: Train loss: 1.1019552946090698, Validation loss: 1.1425529718399048'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 7000: Train loss: 1.0875520706176758, Validation loss: 1.1322033405303955'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 8000: Train loss: 1.0744813680648804, Validation loss: 1.1262868642807007'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 9000: Train loss: 1.0662282705307007, Validation loss: 1.1192575693130493'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigram_language_model_with_attention = BigramLanguageModelWithAttention(\n",
    "    vocab_size, context_size\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(bigram_language_model_with_attention.parameters(), lr=learning_rate)\n",
    "\n",
    "max_iterations = 10000\n",
    "eval_iterations = 1000\n",
    "\n",
    "for iteration in tqdm_notebook(range(max_iterations)):\n",
    "\n",
    "    if iteration % eval_iterations == 0:\n",
    "        evaluated_loss = estimate_loss(bigram_language_model_with_attention, eval_iterations)\n",
    "        display(f'Step {iteration}: Train loss: {evaluated_loss[SplitType.train]}, Validation loss: {evaluated_loss[SplitType.validation]}')\n",
    "\n",
    "    inputs, targets = get_batch(SplitType.train)\n",
    "\n",
    "    logits, loss = bigram_language_model_with_attention(inputs, targets)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m(bigram_language_model_with_attention, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate' is not defined"
     ]
    }
   ],
   "source": [
    "generated_text = generate(bigram_language_model_with_attention, max_new_tokens=1000)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
