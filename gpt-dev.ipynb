{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import List, Tuple, Optional, Dict, Type\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ApoteosisWhatsAppChat.txt', 'r') as f:\n",
    "    input_text = f.read()\n",
    "\n",
    "print(input_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique characters in the text\n",
    "chars_in_text = sorted(list(set(input_text)))\n",
    "vocab_size = len(chars_in_text)\n",
    "display(''.join(chars_in_text))\n",
    "display(f'Total unique characters: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map from chars to int, and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_int = {}\n",
    "int_to_char = {}\n",
    "\n",
    "for i, c in enumerate(chars_in_text):\n",
    "    char_to_int[c] = i\n",
    "    int_to_char[i] = c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to encode and decode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str) -> List[int]:\n",
    "    return [char_to_int[c] for c in text]\n",
    "\n",
    "def decode(encoded_text: List[int]) -> str:\n",
    "    return ''.join([int_to_char[i] for i in encoded_text])\n",
    "\n",
    "# Test them\n",
    "encoded_text = encode('Mi mama me mima')\n",
    "print(encoded_text)\n",
    "\n",
    "print(decode(encoded_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the entire text and put it in a tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tensor = torch.LongTensor(encode(input_text))\n",
    "display(text_tensor.shape)\n",
    "display(text_tensor[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Using 90% of the text as training, the rest as validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = int(0.9 * len(text_tensor))\n",
    "train_data = text_tensor[:split_point]\n",
    "validation_data = text_tensor[split_point:]\n",
    "\n",
    "display(train_data[:100])\n",
    "display(validation_data[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "# Make results reproduceable\n",
    "torch.manual_seed(8888)\n",
    "batch_size = 256\n",
    "# Maximum context length for predictions\n",
    "context_size = 256\n",
    "learning_rate = 3e-4\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class SplitType(Enum):\n",
    "    train = 'TRAIN'\n",
    "    validation = 'VALIDATION'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split_type: SplitType) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    working_data = train_data if split_type == SplitType.train else validation_data\n",
    "    indices = torch.randint(len(working_data) - context_size, (batch_size, ))\n",
    "    inputs = torch.stack([working_data[i : i + context_size] for i in indices])\n",
    "    targets = torch.stack([working_data[i + 1 : i + context_size + 1] for i in indices])\n",
    "    \n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model: Type[nn.Module], eval_iterations: int) -> Dict[float, float]:\n",
    "    estimated_loss = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split_type in SplitType:\n",
    "        split_loss = torch.zeros(eval_iterations)\n",
    "        for k in range(eval_iterations):\n",
    "            inputs, targets = get_batch(split_type)\n",
    "            _, loss = model(inputs, targets)\n",
    "            split_loss[k] = loss.item()\n",
    "        \n",
    "        estimated_loss[split_type] = split_loss.mean()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return estimated_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First try with Bigrams\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        # Embedding table\n",
    "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            inputs: torch.Tensor,\n",
    "            targets: Optional[torch.Tensor] = None\n",
    "            ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "\n",
    "        logits = self.embedding_table(inputs)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if targets is not None:\n",
    "            # Reshape as Pytorch expects\n",
    "            b, t, c = logits.shape\n",
    "            logits = logits.view(b*t, c)\n",
    "            targets = targets.view(b*t)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # The return value is (batch_size, context_size, vocab_size)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generates up to max_new_tokens of text by predicting what comes after the inputs\n",
    "        \"\"\"\n",
    "        prediction_appended = inputs\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(prediction_appended)\n",
    "            # We are interested only in the last time step\n",
    "            logits = logits[:, -1, :]\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            # Sample\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            # Append to the current input\n",
    "            prediction_appended = torch.cat((prediction_appended, next_token), dim=1)\n",
    "\n",
    "        return prediction_appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_language_model = BigramLanguageModel(vocab_size).to(device)\n",
    "inputs, targets = get_batch(SplitType.train)\n",
    "output, loss = bigram_language_model(inputs, targets)\n",
    "display(output.shape, loss)\n",
    "display(f'Expected loss: {-np.log(1/vocab_size)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model: Type[nn.Module], max_new_tokens: int = 100) -> str:\n",
    "    inputs = torch.ones((1, 1), dtype=torch.long).to(device)\n",
    "    encoded_generated_text = model.generate(inputs, max_new_tokens=max_new_tokens)\n",
    "    return decode(encoded_generated_text[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate!\n",
    "generate(bigram_language_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "optimizer = torch.optim.AdamW(bigram_language_model.parameters(), lr=1e-3)\n",
    "\n",
    "max_iterations = 10000\n",
    "eval_iterations = 1000\n",
    "\n",
    "for iteration in tqdm_notebook(range(max_iterations)):\n",
    "\n",
    "    if iteration % eval_iterations == 0:\n",
    "        evaluated_loss = estimate_loss(bigram_language_model, eval_iterations)\n",
    "        display(f'Step {iteration}: Train loss: {evaluated_loss[SplitType.train]}, Validation loss: {evaluated_loss[SplitType.validation]}')\n",
    "\n",
    "    inputs, targets = get_batch(SplitType.train)\n",
    "\n",
    "    logits, loss = bigram_language_model(inputs, targets)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(bigram_language_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty much the limit of the model without tinkering anything else.\n",
    "\n",
    "Now, let's plug attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTENTION_HEAD_DIMENSIONS = 384\n",
    "dropout_rate = 0.2\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embedding_dimensions: int, head_dimensions: int):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embedding_dimensions, head_dimensions)\n",
    "        self.query = nn.Linear(n_embedding_dimensions, head_dimensions)\n",
    "        self.value = nn.Linear(n_embedding_dimensions, head_dimensions)\n",
    "\n",
    "        # Since this is not a parameter, it's registered as a buffer\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, time_pos, dimensions = inputs.shape\n",
    "        key = self.key(inputs)\n",
    "        query = self.query(inputs)\n",
    "\n",
    "        # Calculating affinities\n",
    "        affinities = query @ key.transpose(-2, -1) * dimensions ** -0.5\n",
    "        affinities = affinities.masked_fill(self.tril[:time_pos, :time_pos] == 0, float('-inf'))\n",
    "        affinities = F.softmax(affinities, dim=-1)\n",
    "        affinities = self.dropout(affinities)\n",
    "\n",
    "        value = self.value(inputs)\n",
    "        outputs = affinities @ value\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads: int, n_embedding_dimensions: int, attention_head_dimensions: int) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(n_embedding_dimensions, attention_head_dimensions) for _ in range(n_heads)] \n",
    "        )\n",
    "        self.projection = nn.Linear(attention_head_dimensions * n_heads, attention_head_dimensions * n_heads)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, inputs) -> torch.Tensor:\n",
    "        outputs = torch.cat([head(inputs) for head in self.heads], dim=-1)\n",
    "        outputs = self.projection(outputs)\n",
    "        outputs = self.dropout(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, attention_head_dimensions: int) -> None:\n",
    "        super().__init__()\n",
    "        self.forward_net = nn.Sequential(\n",
    "            nn.Linear(attention_head_dimensions, 4 * attention_head_dimensions),\n",
    "            nn.ReLU(),\n",
    "            # Projection layer\n",
    "            nn.Linear(4 * attention_head_dimensions, attention_head_dimensions),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward_net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A transformer block for communication -> computation\"\"\"\n",
    "\n",
    "    def __init__(self, attention_head_dimensions: int, n_attention_heads: int, n_embedding_dimensions: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = attention_head_dimensions // n_attention_heads\n",
    "        self.self_attention_heads = MultiHeadAttention(\n",
    "            n_attention_heads, n_embedding_dimensions, head_size\n",
    "        )\n",
    "        self.forward_net = FeedForward(attention_head_dimensions)\n",
    "        self.layer_norm1 = nn.LayerNorm(attention_head_dimensions)\n",
    "        self.layer_norm2 = nn.LayerNorm(attention_head_dimensions)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        # The addition is for residual connections\n",
    "        outputs = outputs + self.self_attention_heads(self.layer_norm1(inputs))\n",
    "        outputs = outputs + self.forward_net(self.layer_norm2(outputs))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelWithAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size: int, \n",
    "            context_size: int,\n",
    "            n_embedding_dimensions: int = 384,\n",
    "            n_attention_heads: int = 6,\n",
    "            n_transformer_blocks: int = 6\n",
    "        ):\n",
    "        super().__init__()\n",
    "        # Embedding table\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embedding_dimensions)\n",
    "        # Position as embedding. Transformers don't have a means to know which element of a sequence\n",
    "        # they are working with. This is learned in this table\n",
    "        self.position_embedding_table = nn.Embedding(context_size, n_embedding_dimensions)\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(ATTENTION_HEAD_DIMENSIONS, n_attention_heads, n_embedding_dimensions)\n",
    "                for _ in range(n_transformer_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(ATTENTION_HEAD_DIMENSIONS)\n",
    " \n",
    "        self.self_attention_heads = MultiHeadAttention(\n",
    "            n_attention_heads, n_embedding_dimensions, ATTENTION_HEAD_DIMENSIONS // n_attention_heads\n",
    "        )\n",
    "        self.forward_net = FeedForward(ATTENTION_HEAD_DIMENSIONS)\n",
    "        # Linear layer to go from embeddings to logits\n",
    "        self.lm_head = nn.Linear(ATTENTION_HEAD_DIMENSIONS, vocab_size)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            inputs: torch.Tensor,\n",
    "            targets: Optional[torch.Tensor] = None\n",
    "        ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \n",
    "        batch_size, time_pos = inputs.shape\n",
    "        token_embeddings = self.embedding_table(inputs)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(time_pos, device=device))\n",
    "        head_inputs = token_embeddings + position_embeddings\n",
    "        head_inputs = self.self_attention_heads(head_inputs)\n",
    "        head_inputs = self.forward_net(head_inputs)\n",
    "        logits = self.lm_head(head_inputs)  # (batch size, time, embedding_dimensions)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if targets is not None:\n",
    "            # Reshape as Pytorch expects\n",
    "            b, t, c = logits.shape\n",
    "            logits = logits.view(b*t, c)\n",
    "            targets = targets.view(b*t)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # The return value is (batch_size, context_size, vocab_size)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generates up to max_new_tokens of text by predicting what comes after the inputs\n",
    "        \"\"\"\n",
    "        prediction_appended = inputs\n",
    "        for _ in range(max_new_tokens):\n",
    "            inputs_cropped = prediction_appended[:, -context_size:]\n",
    "            logits, _ = self(inputs_cropped)\n",
    "            # We are interested only in the last time step\n",
    "            logits = logits[:, -1, :]\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            # Sample\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            # Append to the current input\n",
    "            prediction_appended = torch.cat((prediction_appended, next_token), dim=1)\n",
    "\n",
    "        return prediction_appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_language_model_with_attention = BigramLanguageModelWithAttention(\n",
    "    vocab_size, context_size\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(bigram_language_model_with_attention.parameters(), lr=learning_rate)\n",
    "\n",
    "max_iterations = 10000\n",
    "eval_iterations = 1000\n",
    "\n",
    "for iteration in tqdm_notebook(range(max_iterations)):\n",
    "\n",
    "    if iteration % eval_iterations == 0:\n",
    "        evaluated_loss = estimate_loss(bigram_language_model_with_attention, eval_iterations)\n",
    "        display(f'Step {iteration}: Train loss: {evaluated_loss[SplitType.train]}, Validation loss: {evaluated_loss[SplitType.validation]}')\n",
    "\n",
    "    inputs, targets = get_batch(SplitType.train)\n",
    "\n",
    "    logits, loss = bigram_language_model_with_attention(inputs, targets)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate(bigram_language_model_with_attention, max_new_tokens=1000)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
