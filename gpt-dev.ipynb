{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import List, Tuple, Optional, Dict, Type\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexandro Barajas: Image_Placeholder\n",
      "Gustavo Fuentes: changos, no pos en definitiva si necesitas una de esas\n",
      "Alexandro Barajas: Image_Placeholder\n",
      "Gustavo Fuentes: 🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣\n",
      "Omar Valerdi: Image_Placeholder\n",
      "Alexandro Barajas: Image_Placeholder\n",
      "Gustavo Fuentes: Image_Placeholder\n",
      "Gustavo Fuentes: https://youtu.be/SFj9u42et7o?t=189\n",
      "Gustavo Fuentes: @818052144488 por qué les ponen \"chips\" al arroz?\n",
      "Omar Bazavilvazo: 1) nadie en la vida le pone chips. Al arroz . Hay algo llamado furikake para los niños\n",
      "Omar Bazavilvazo: 2) jamón? Letra en romanji? Wow nunca conocí un japonés que me invitara algo con jamón , que paciencia de hacer eso … pero sabe Manuel tiene más tiempo allá , capaz que ya cambiaron las cosas\n",
      "Ignacio Pérez Bermúdez: Image_Placeholder\n",
      "Gustavo Fuentes: No debe ser tan incomun ya que hay un utensilio para triturar chips\n",
      "Gustavo Fuentes: Las letras me imagino que son novelty\n",
      "Omar Valerdi: Image_Placeholder\n",
      "Ignacio Pérez Bermúdez: Image_Placeholder\n",
      "Alexandro Barajas: Que esperant\n"
     ]
    }
   ],
   "source": [
    "with open('ApoteosisWhatsAppChat.txt', 'r') as f:\n",
    "    input_text = f.read()\n",
    "\n",
    "print(input_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t\\n !\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz|}~\\xa0¡ª«°´»¿ÁÉÍÑÓÚàáäçéíîñóö÷úüĺńŕś\\u200b\\u200d\\u200e—‘’“”•…\\u202f‼⁴₉↑√∧▶◇☝☠☹☺♂♥♬⚡✅✋✌✔❌❤、。あいうがこさしすただっつでとどなにのはほまみょりるれをんァアイエオキグケコゴシジスセッテデトネパヒビフプマメラルンー一七三上中了井交人什付件位低個億削化卑和哥地報墨変子字学少屋崎常平年当悟悪情愛懸投抹指文日更最木本朱杉根沢泊波活流淳添漏漢然生用痛発目石票空米素編置肉芸英茶菓西見語読賞輪造道選酸鈴院除集電露音風️！（）？𝑪𝒂𝒄𝒉𝒊𝒐𝒔𝒕𝘼𝙒𝙖𝙚𝙝𝙡𝙩🇧🇪🇲🇷🇸🇽🌲🌸🌹🍎🍔🍕🍰🍷🍺🍻🍾🍿🎁🎂🎄🎅🎈🎉🎊🏻🏼🏽🏾🐢🐧🐯🐶🐷🐻👀👁👆👉👌👍👎👏👨👴👻👽💊💜💣💩💫💯💰📍📸🔥🖐😀😁😂😃😄😅😆😈😉😊😋😌😍😎😏😐😑😒😓😔😕😖😛😜😝😞😠😡😢😣😤😦😧😨😫😬😭😮😰😱😲😳😴😵😶🙂🙃🙄🙇🙈🙉🙊🙋🙏🚀🚧🚨🚬🛑🤐🤓🤔🤗🤘🤟🤡🤢🤣🤤🤦🤨🤩🤪🤬🤭🤮🤯🤷🥂🥃🥚🥰🥱🥲🥳🥵🥶🥹🦁🧁🧐🪄🪅🫂🫢🫤🫥'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Total unique characters: 492'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the unique characters in the text\n",
    "chars_in_text = sorted(list(set(input_text)))\n",
    "vocab_size = len(chars_in_text)\n",
    "display(''.join(chars_in_text))\n",
    "display(f'Total unique characters: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map from chars to int, and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_int = {}\n",
    "int_to_char = {}\n",
    "\n",
    "for i, c in enumerate(chars_in_text):\n",
    "    char_to_int[c] = i\n",
    "    int_to_char[i] = c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to encode and decode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 75, 2, 79, 67, 79, 67, 2, 79, 71, 2, 79, 75, 79, 67]\n",
      "Mi mama me mima\n"
     ]
    }
   ],
   "source": [
    "def encode(text: str) -> List[int]:\n",
    "    return [char_to_int[c] for c in text]\n",
    "\n",
    "def decode(encoded_text: List[int]) -> str:\n",
    "    return ''.join([int_to_char[i] for i in encoded_text])\n",
    "\n",
    "# Test them\n",
    "encoded_text = encode('Mi mama me mima')\n",
    "print(encoded_text)\n",
    "\n",
    "print(decode(encoded_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the entire text and put it in a tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2461096])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([35, 78, 71, 90, 67, 80, 70, 84, 81,  2, 36, 67, 84, 67, 76, 67, 85, 28,\n",
       "         2, 43, 79, 67, 73, 71, 65, 50, 78, 67, 69, 71, 74, 81, 78, 70, 71, 84,\n",
       "         1, 41, 87, 85, 86, 67, 88, 81,  2, 40, 87, 71, 80, 86, 71, 85, 28,  2,\n",
       "        69, 74, 67, 80, 73, 81, 85, 14,  2, 80, 81,  2, 82, 81, 85,  2, 71, 80,\n",
       "         2, 70, 71, 72, 75, 80, 75, 86, 75, 88, 67,  2, 85, 75,  2, 80, 71, 69,\n",
       "        71, 85, 75, 86, 67, 85,  2, 87, 80, 67])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_tensor = torch.LongTensor(encode(input_text))\n",
    "display(text_tensor.shape)\n",
    "display(text_tensor[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Using 90% of the text as training, the rest as validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35, 78, 71, 90, 67, 80, 70, 84, 81,  2, 36, 67, 84, 67, 76, 67, 85, 28,\n",
       "         2, 43, 79, 67, 73, 71, 65, 50, 78, 67, 69, 71, 74, 81, 78, 70, 71, 84,\n",
       "         1, 41, 87, 85, 86, 67, 88, 81,  2, 40, 87, 71, 80, 86, 71, 85, 28,  2,\n",
       "        69, 74, 67, 80, 73, 81, 85, 14,  2, 80, 81,  2, 82, 81, 85,  2, 71, 80,\n",
       "         2, 70, 71, 72, 75, 80, 75, 86, 75, 88, 67,  2, 85, 75,  2, 80, 71, 69,\n",
       "        71, 85, 75, 86, 67, 85,  2, 87, 80, 67])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 71,  76,  67,  80,  70,  84,  81,   2,  10,  45,  75,  79,  11,   2,\n",
       "         50, 114,  84,  71,  92,  28,   2,  54,  81,  70,  81,   2,  71,  85,\n",
       "         86,  67,   2,  74,  71,  69,  74,  81,   2,  69,  81,  80,   2,  78,\n",
       "         67,  85,   2,  82,  67,  86,  67,  85,  14,   2,  83,  87, 114,   2,\n",
       "         74,  67,  91,   2,  69,  81,  79,  82,  78,  71,  86,  81,   2,  81,\n",
       "          2,  83,  87,  71,   2,  72,  87,  80,  69,  75,  81,  80,  71,   2,\n",
       "         68,  75,  71,  80,  33,   1,  41,  87,  85,  86,  67,  88,  81,   2,\n",
       "         40,  87])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split_point = int(0.9 * len(text_tensor))\n",
    "train_data = text_tensor[:split_point]\n",
    "validation_data = text_tensor[split_point:]\n",
    "\n",
    "display(train_data[:100])\n",
    "display(validation_data[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "# Make results reproduceable\n",
    "torch.manual_seed(8888)\n",
    "batch_size = 256\n",
    "# Maximum context length for predictions\n",
    "context_size = 256\n",
    "learning_rate = 3e-4\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class SplitType(Enum):\n",
    "    train = 'TRAIN'\n",
    "    validation = 'VALIDATION'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split_type: SplitType) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    working_data = train_data if split_type == SplitType.train else validation_data\n",
    "    indices = torch.randint(len(working_data) - context_size, (batch_size, ))\n",
    "    inputs = torch.stack([working_data[i : i + context_size] for i in indices])\n",
    "    targets = torch.stack([working_data[i + 1 : i + context_size + 1] for i in indices])\n",
    "    \n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model: Type[nn.Module], eval_iterations: int) -> Dict[float, float]:\n",
    "    estimated_loss = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split_type in SplitType:\n",
    "        split_loss = torch.zeros(eval_iterations)\n",
    "        for k in range(eval_iterations):\n",
    "            inputs, targets = get_batch(split_type)\n",
    "            _, loss = model(inputs, targets)\n",
    "            split_loss[k] = loss.item()\n",
    "        \n",
    "        estimated_loss[split_type] = split_loss.mean()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return estimated_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First try with Bigrams\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        # Embedding table\n",
    "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            inputs: torch.Tensor,\n",
    "            targets: Optional[torch.Tensor] = None\n",
    "            ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "\n",
    "        logits = self.embedding_table(inputs)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if targets is not None:\n",
    "            # Reshape as Pytorch expects\n",
    "            b, t, c = logits.shape\n",
    "            logits = logits.view(b*t, c)\n",
    "            targets = targets.view(b*t)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # The return value is (batch_size, context_size, vocab_size)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generates up to max_new_tokens of text by predicting what comes after the inputs\n",
    "        \"\"\"\n",
    "        prediction_appended = inputs\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(prediction_appended)\n",
    "            # We are interested only in the last time step\n",
    "            logits = logits[:, -1, :]\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            # Sample\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            # Append to the current input\n",
    "            prediction_appended = torch.cat((prediction_appended, next_token), dim=1)\n",
    "\n",
    "        return prediction_appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65536, 492])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(6.6914, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Expected loss: 6.198478716492308'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigram_language_model = BigramLanguageModel(vocab_size).to(device)\n",
    "inputs, targets = get_batch(SplitType.train)\n",
    "output, loss = bigram_language_model(inputs, targets)\n",
    "display(output.shape, loss)\n",
    "display(f'Expected loss: {-np.log(1/vocab_size)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model: Type[nn.Module], max_new_tokens: int = 100) -> str:\n",
    "    inputs = torch.ones((1, 1), dtype=torch.long).to(device)\n",
    "    encoded_generated_text = model.generate(inputs, max_new_tokens=max_new_tokens)\n",
    "    return decode(encoded_generated_text[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t🍻♂5🍾(プé+h👁Vいg”根常,🐯&キ𝒊!✋áÚ🫢i(プ🐶k_屋🎉🍻🙊😈つA風;流井セ✔付🐷語3😄プMア👽ś石🤡𝒄f😨？d七☠英\\xa0W🚧🥵👎平🙊更✔😁😕発😆\\t😕}スaú?🎂😊p😨い𝙝😁波たd当XYÑ$'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate!\n",
    "generate(bigram_language_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033bc45042b343fba79099bae3f13ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 0: Train loss: 5.624845504760742, Validation loss: 5.623149871826172'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 1000: Train loss: 4.147069454193115, Validation loss: 4.134329795837402'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 2000: Train loss: 3.2229230403900146, Validation loss: 3.199406147003174'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 3000: Train loss: 2.760833978652954, Validation loss: 2.7316036224365234'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 4000: Train loss: 2.5586998462677, Validation loss: 2.527670383453369'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 5000: Train loss: 2.4757778644561768, Validation loss: 2.443186044692993'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 6000: Train loss: 2.438324451446533, Validation loss: 2.406372547149658'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 7000: Train loss: 2.4204578399658203, Validation loss: 2.38958740234375'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 8000: Train loss: 2.4117178916931152, Validation loss: 2.3797426223754883'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 9000: Train loss: 2.4053385257720947, Validation loss: 2.3742458820343018'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "optimizer = torch.optim.AdamW(bigram_language_model.parameters(), lr=1e-3)\n",
    "\n",
    "max_iterations = 10000\n",
    "eval_iterations = 1000\n",
    "\n",
    "for iteration in tqdm_notebook(range(max_iterations)):\n",
    "\n",
    "    if iteration % eval_iterations == 0:\n",
    "        evaluated_loss = estimate_loss(bigram_language_model, eval_iterations)\n",
    "        display(f'Step {iteration}: Train loss: {evaluated_loss[SplitType.train]}, Validation loss: {evaluated_loss[SplitType.validation]}')\n",
    "\n",
    "    inputs, targets = get_batch(SplitType.train)\n",
    "\n",
    "    logits, loss = bigram_language_model(inputs, targets)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t1?\\nMardrstavant🤨‼𝒐ージは編集さされました\\nT&s: ajant\\nAlldehe ndr Ales eho 😝😭🔥なŕ𝑪😤💫。Jabese a ma.\\nMFuevoue gez Po '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate(bigram_language_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty much the limit of the model without tinkering anything else.\n",
    "\n",
    "Now, let's plug attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTENTION_HEAD_DIMENSIONS = 384\n",
    "dropout_rate = 0.2\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embedding_dimensions: int, head_dimensions: int):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embedding_dimensions, head_dimensions)\n",
    "        self.query = nn.Linear(n_embedding_dimensions, head_dimensions)\n",
    "        self.value = nn.Linear(n_embedding_dimensions, head_dimensions)\n",
    "\n",
    "        # Since this is not a parameter, it's registered as a buffer\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, time_pos, dimensions = inputs.shape\n",
    "        key = self.key(inputs)\n",
    "        query = self.query(inputs)\n",
    "\n",
    "        # Calculating affinities\n",
    "        affinities = query @ key.transpose(-2, -1) * dimensions ** -0.5\n",
    "        affinities = affinities.masked_fill(self.tril[:time_pos, :time_pos] == 0, float('-inf'))\n",
    "        affinities = F.softmax(affinities, dim=-1)\n",
    "        affinities = self.dropout(affinities)\n",
    "\n",
    "        value = self.value(inputs)\n",
    "        outputs = affinities @ value\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads: int, n_embedding_dimensions: int, attention_head_dimensions: int) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(n_embedding_dimensions, attention_head_dimensions) for _ in range(n_heads)] \n",
    "        )\n",
    "        self.projection = nn.Linear(attention_head_dimensions * n_heads, attention_head_dimensions * n_heads)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, inputs) -> torch.Tensor:\n",
    "        outputs = torch.cat([head(inputs) for head in self.heads], dim=-1)\n",
    "        outputs = self.projection(outputs)\n",
    "        outputs = self.dropout(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, attention_head_dimensions: int) -> None:\n",
    "        super().__init__()\n",
    "        self.forward_net = nn.Sequential(\n",
    "            nn.Linear(attention_head_dimensions, 4 * attention_head_dimensions),\n",
    "            nn.ReLU(),\n",
    "            # Projection layer\n",
    "            nn.Linear(4 * attention_head_dimensions, attention_head_dimensions),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward_net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A transformer block for communication -> computation\"\"\"\n",
    "\n",
    "    def __init__(self, attention_head_dimensions: int, n_attention_heads: int, n_embedding_dimensions: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = attention_head_dimensions // n_attention_heads\n",
    "        self.self_attention_heads = MultiHeadAttention(\n",
    "            n_attention_heads, n_embedding_dimensions, head_size\n",
    "        )\n",
    "        self.forward_net = FeedForward(attention_head_dimensions)\n",
    "        self.layer_norm1 = nn.LayerNorm(attention_head_dimensions)\n",
    "        self.layer_norm2 = nn.LayerNorm(attention_head_dimensions)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        # The addition is for residual connections\n",
    "        outputs = outputs + self.self_attention_heads(self.layer_norm1(inputs))\n",
    "        outputs = outputs + self.forward_net(self.layer_norm2(outputs))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelWithAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size: int, \n",
    "            context_size: int,\n",
    "            n_embedding_dimensions: int = 384,\n",
    "            n_attention_heads: int = 6,\n",
    "            n_transformer_blocks: int = 6\n",
    "        ):\n",
    "        super().__init__()\n",
    "        # Embedding table\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embedding_dimensions)\n",
    "        # Position as embedding. Transformers don't have a means to know which element of a sequence\n",
    "        # they are working with. This is learned in this table\n",
    "        self.position_embedding_table = nn.Embedding(context_size, n_embedding_dimensions)\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(ATTENTION_HEAD_DIMENSIONS, n_attention_heads, n_embedding_dimensions)\n",
    "                for _ in range(n_transformer_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(ATTENTION_HEAD_DIMENSIONS)\n",
    " \n",
    "        self.self_attention_heads = MultiHeadAttention(\n",
    "            n_attention_heads, n_embedding_dimensions, ATTENTION_HEAD_DIMENSIONS // n_attention_heads\n",
    "        )\n",
    "        self.forward_net = FeedForward(ATTENTION_HEAD_DIMENSIONS)\n",
    "        # Linear layer to go from embeddings to logits\n",
    "        self.lm_head = nn.Linear(ATTENTION_HEAD_DIMENSIONS, vocab_size)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            inputs: torch.Tensor,\n",
    "            targets: Optional[torch.Tensor] = None\n",
    "        ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \n",
    "        batch_size, time_pos = inputs.shape\n",
    "        token_embeddings = self.embedding_table(inputs)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(time_pos, device=device))\n",
    "        head_inputs = token_embeddings + position_embeddings\n",
    "        head_inputs = self.self_attention_heads(head_inputs)\n",
    "        head_inputs = self.forward_net(head_inputs)\n",
    "        logits = self.lm_head(head_inputs)  # (batch size, time, embedding_dimensions)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if targets is not None:\n",
    "            # Reshape as Pytorch expects\n",
    "            b, t, c = logits.shape\n",
    "            logits = logits.view(b*t, c)\n",
    "            targets = targets.view(b*t)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # The return value is (batch_size, context_size, vocab_size)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generates up to max_new_tokens of text by predicting what comes after the inputs\n",
    "        \"\"\"\n",
    "        prediction_appended = inputs\n",
    "        for _ in range(max_new_tokens):\n",
    "            inputs_cropped = prediction_appended[:, -context_size:]\n",
    "            logits, _ = self(inputs_cropped)\n",
    "            # We are interested only in the last time step\n",
    "            logits = logits[:, -1, :]\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            # Sample\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            # Append to the current input\n",
    "            prediction_appended = torch.cat((prediction_appended, next_token), dim=1)\n",
    "\n",
    "        return prediction_appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f03827669b4862998e7e75c4100025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 0: Train loss: 6.191287040710449, Validation loss: 6.191257476806641'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 1000: Train loss: 1.4562420845031738, Validation loss: 1.4608640670776367'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 2000: Train loss: 1.2663781642913818, Validation loss: 1.2786730527877808'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 3000: Train loss: 1.1884504556655884, Validation loss: 1.2107690572738647'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 4000: Train loss: 1.1465976238250732, Validation loss: 1.1751075983047485'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 5000: Train loss: 1.1217700242996216, Validation loss: 1.1556854248046875'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 6000: Train loss: 1.1019552946090698, Validation loss: 1.1425529718399048'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 7000: Train loss: 1.0875520706176758, Validation loss: 1.1322033405303955'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 8000: Train loss: 1.0744813680648804, Validation loss: 1.1262868642807007'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Step 9000: Train loss: 1.0662282705307007, Validation loss: 1.1192575693130493'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigram_language_model_with_attention = BigramLanguageModelWithAttention(\n",
    "    vocab_size, context_size\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(bigram_language_model_with_attention.parameters(), lr=learning_rate)\n",
    "\n",
    "max_iterations = 10000\n",
    "eval_iterations = 1000\n",
    "\n",
    "for iteration in tqdm_notebook(range(max_iterations)):\n",
    "\n",
    "    if iteration % eval_iterations == 0:\n",
    "        evaluated_loss = estimate_loss(bigram_language_model_with_attention, eval_iterations)\n",
    "        display(f'Step {iteration}: Train loss: {evaluated_loss[SplitType.train]}, Validation loss: {evaluated_loss[SplitType.validation]}')\n",
    "\n",
    "    inputs, targets = get_batch(SplitType.train)\n",
    "\n",
    "    logits, loss = bigram_language_model_with_attention(inputs, targets)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m(bigram_language_model_with_attention, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate' is not defined"
     ]
    }
   ],
   "source": [
    "generated_text = generate(bigram_language_model_with_attention, max_new_tokens=1000)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
